{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# hyperparameters\n",
    "# hyperparameters\n",
    "batch_size = 64  # nie ruszam\n",
    "block_size = 128  # nie ruszam\n",
    "max_iters = 6000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "\n",
    "print(block_size)\n",
    "torch.manual_seed(166045)\n",
    "\n",
    "# Load text data\n",
    "with open('pan-tadeusz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split, seq_sampling=False):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    if seq_sampling:\n",
    "        start_idx = torch.randint(0, len(data) - block_size, (1,)).item()\n",
    "        x = data[start_idx:start_idx + block_size].unsqueeze(0).repeat(batch_size, 1)\n",
    "        y = data[start_idx + 1:start_idx + block_size + 1].unsqueeze(0).repeat(batch_size, 1)\n",
    "    else:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "        y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.sa(self.ln1(x)))\n",
    "        x = x + self.dropout(self.ffwd(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "def calculate_perplexity(logits, targets):\n",
    "    # Przekształć logits na logarytmy prawdopodobieństw (softmax + log)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Spłaszcz targets do wymiaru 1D, aby pasował do logits\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # Wybierz logarytmy prawdopodobieństw odpowiadające poprawnym tokenom (targets)\n",
    "    log_probs_target = log_probs[torch.arange(logits.size(0)), targets]\n",
    "\n",
    "    # Oblicz średni logarytm prawdopodobieństwa na token\n",
    "    mean_log_prob = log_probs_target.mean()\n",
    "\n",
    "    # Oblicz perplexity na podstawie wzoru\n",
    "    perplexity = torch.exp(-mean_log_prob).item()\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = GPTModel().to(device)\n",
    "# print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Lists to store metrics\n",
    "# train_losses, val_losses, perplexities_train, perplexities_val = [], [], [], []\n",
    "\n",
    "# # Training loop\n",
    "# for iter in range(max_iters):\n",
    "#     # Collect training metrics for every iteration\n",
    "#     xb, yb = get_batch('train')\n",
    "#     logits, loss = model(xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Save training loss and perplexity for every iteration\n",
    "#     train_losses.append(loss.item())\n",
    "#     perplexities_train.append(calculate_perplexity(logits, yb))\n",
    "\n",
    "#     # Collect validation metrics for every iteration\n",
    "#     X_val, Y_val = get_batch('val')\n",
    "#     logits_val, _ = model(X_val, Y_val)\n",
    "#     val_loss = F.cross_entropy(logits_val.view(-1, logits_val.size(-1)), Y_val.view(-1)).item()\n",
    "#     val_losses.append(val_loss)\n",
    "#     perplexity_val = calculate_perplexity(logits_val, Y_val)\n",
    "#     perplexities_val.append(perplexity_val)\n",
    "\n",
    "#     # Print metrics at eval_interval or the last iteration\n",
    "#     if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "#         print(f\"step {iter}: train loss {train_losses[-1]:.4f}, val loss {val_losses[-1]:.4f}, \"\n",
    "#               f\"train perplexity {perplexities_train[-1]:.4f}, val perplexity {perplexity_val:.4f}\")\n",
    "        \n",
    "# # Create the directory if it doesn't exist\n",
    "# output_dir = \"wykresy\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Plot training and validation perplexity on the same graph\n",
    "# plt.figure()\n",
    "# plt.plot(perplexities_train, label='Nieokreśloność zbiór treningowy', color='blue')\n",
    "# plt.plot(perplexities_val, label='Nieokreśloność zbiór walidacyjny', color='orange')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Epoka')\n",
    "# plt.ylabel('Nieokreśloność')\n",
    "# plt.title('Zbiór treningowy vs walidacyjny')\n",
    "# # Save the plot\n",
    "# plt.savefig(os.path.join(output_dir, f'nieokreslonosc_{batch_size}.png'))\n",
    "# plt.close()\n",
    "\n",
    "# # Plot training loss\n",
    "# plt.figure()\n",
    "# plt.plot(train_losses, label='Strata zbiór treningowy', color='blue')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteracja')\n",
    "# plt.ylabel('Strata')\n",
    "# plt.title('Strata treningowa w czasie')\n",
    "# # Save the plot\n",
    "# plt.savefig(os.path.join(output_dir, f'strata_treningowa_{batch_size}.png'))\n",
    "# plt.close()\n",
    "\n",
    "# # Plot validation loss\n",
    "# plt.figure()\n",
    "# plt.plot(val_losses, label='Strata zbiór walidacyjny', color='orange')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Iteracja')\n",
    "# plt.ylabel('Strata')\n",
    "# plt.title('Strata walidacyjna w czasie')\n",
    "# # Save the plot\n",
    "# plt.savefig(os.path.join(output_dir, f'strata_walidacyjna_{batch_size}.png'))\n",
    "# plt.close()\n",
    "\n",
    "# # Generate text\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# generated_text = decode(model.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "\n",
    "# # Save generated text to a file\n",
    "# with open(f'wygenerowany_tekst_{batch_size}_{n_embd}_dropout_{dropout}.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with config: n_embd=256, n_head=8, n_layer=8, dropout=0.2, learning rate = 0.0003\n",
      "step 0: train loss 4.6067, val loss 3.9707, train perplexity 100.1533, val perplexity 53.0210\n",
      "step 100: train loss 2.5864, val loss 2.5980, train perplexity 13.2823, val perplexity 13.4370\n",
      "step 200: train loss 2.5429, val loss 2.5288, train perplexity 12.7163, val perplexity 12.5384\n",
      "step 300: train loss 2.5081, val loss 2.5359, train perplexity 12.2818, val perplexity 12.6280\n",
      "step 400: train loss 2.4629, val loss 2.4864, train perplexity 11.7385, val perplexity 12.0179\n",
      "step 500: train loss 2.3904, val loss 2.4192, train perplexity 10.9179, val perplexity 11.2371\n",
      "step 600: train loss 2.3372, val loss 2.3294, train perplexity 10.3523, val perplexity 10.2722\n",
      "step 700: train loss 2.2811, val loss 2.2674, train perplexity 9.7871, val perplexity 9.6542\n",
      "step 800: train loss 2.2346, val loss 2.2416, train perplexity 9.3427, val perplexity 9.4082\n",
      "step 900: train loss 2.2005, val loss 2.2184, train perplexity 9.0295, val perplexity 9.1925\n",
      "step 1000: train loss 2.1345, val loss 2.1876, train perplexity 8.4528, val perplexity 8.9140\n",
      "step 1100: train loss 2.0796, val loss 2.1551, train perplexity 8.0017, val perplexity 8.6290\n",
      "step 1200: train loss 2.0629, val loss 2.0774, train perplexity 7.8691, val perplexity 7.9833\n",
      "step 1300: train loss 2.0119, val loss 2.0915, train perplexity 7.4772, val perplexity 8.0970\n",
      "step 1400: train loss 1.9838, val loss 2.0542, train perplexity 7.2704, val perplexity 7.8005\n",
      "step 1500: train loss 1.9492, val loss 2.0493, train perplexity 7.0230, val perplexity 7.7627\n",
      "step 1600: train loss 1.9285, val loss 1.9905, train perplexity 6.8790, val perplexity 7.3190\n",
      "step 1700: train loss 1.8821, val loss 1.9710, train perplexity 6.5673, val perplexity 7.1779\n",
      "step 1800: train loss 1.8563, val loss 1.9622, train perplexity 6.3997, val perplexity 7.1150\n",
      "step 1900: train loss 1.8334, val loss 1.9421, train perplexity 6.2552, val perplexity 6.9735\n",
      "step 2000: train loss 1.7902, val loss 1.9324, train perplexity 5.9904, val perplexity 6.9062\n",
      "step 2100: train loss 1.7810, val loss 1.9064, train perplexity 5.9359, val perplexity 6.7285\n",
      "step 2200: train loss 1.7967, val loss 1.8703, train perplexity 6.0297, val perplexity 6.4901\n",
      "step 2300: train loss 1.7777, val loss 1.8537, train perplexity 5.9162, val perplexity 6.3836\n",
      "step 2400: train loss 1.7086, val loss 1.8731, train perplexity 5.5214, val perplexity 6.5086\n",
      "step 2500: train loss 1.7249, val loss 1.8653, train perplexity 5.6117, val perplexity 6.4578\n",
      "step 2600: train loss 1.7391, val loss 1.8414, train perplexity 5.6922, val perplexity 6.3051\n",
      "step 2700: train loss 1.6972, val loss 1.8172, train perplexity 5.4589, val perplexity 6.1545\n",
      "step 2800: train loss 1.6474, val loss 1.8321, train perplexity 5.1936, val perplexity 6.2469\n",
      "step 2900: train loss 1.6718, val loss 1.8334, train perplexity 5.3218, val perplexity 6.2551\n",
      "step 3000: train loss 1.6323, val loss 1.8596, train perplexity 5.1156, val perplexity 6.4212\n",
      "step 3100: train loss 1.6698, val loss 1.8226, train perplexity 5.3111, val perplexity 6.1878\n",
      "step 3200: train loss 1.6474, val loss 1.7902, train perplexity 5.1937, val perplexity 5.9906\n",
      "step 3300: train loss 1.6081, val loss 1.7797, train perplexity 4.9935, val perplexity 5.9278\n",
      "step 3400: train loss 1.5969, val loss 1.8286, train perplexity 4.9376, val perplexity 6.2250\n",
      "step 3500: train loss 1.5930, val loss 1.7854, train perplexity 4.9187, val perplexity 5.9622\n",
      "step 3600: train loss 1.5833, val loss 1.7855, train perplexity 4.8710, val perplexity 5.9627\n",
      "step 3700: train loss 1.5432, val loss 1.7533, train perplexity 4.6795, val perplexity 5.7739\n",
      "step 3800: train loss 1.6011, val loss 1.7703, train perplexity 4.9584, val perplexity 5.8728\n",
      "step 3900: train loss 1.5643, val loss 1.7925, train perplexity 4.7791, val perplexity 6.0042\n",
      "step 4000: train loss 1.5358, val loss 1.7264, train perplexity 4.6448, val perplexity 5.6205\n",
      "step 4100: train loss 1.5358, val loss 1.7398, train perplexity 4.6449, val perplexity 5.6962\n",
      "step 4200: train loss 1.5375, val loss 1.7926, train perplexity 4.6532, val perplexity 6.0053\n",
      "step 4300: train loss 1.5241, val loss 1.7911, train perplexity 4.5910, val perplexity 5.9959\n",
      "step 4400: train loss 1.5341, val loss 1.7819, train perplexity 4.6373, val perplexity 5.9412\n",
      "step 4500: train loss 1.4960, val loss 1.7368, train perplexity 4.4637, val perplexity 5.6793\n",
      "step 4600: train loss 1.5049, val loss 1.7694, train perplexity 4.5037, val perplexity 5.8671\n",
      "step 4700: train loss 1.4934, val loss 1.7793, train perplexity 4.4524, val perplexity 5.9259\n",
      "step 4800: train loss 1.4504, val loss 1.7631, train perplexity 4.2649, val perplexity 5.8308\n",
      "step 4900: train loss 1.4656, val loss 1.7784, train perplexity 4.3302, val perplexity 5.9206\n",
      "step 5000: train loss 1.4668, val loss 1.7798, train perplexity 4.3352, val perplexity 5.9284\n",
      "step 5100: train loss 1.4512, val loss 1.8055, train perplexity 4.2684, val perplexity 6.0829\n",
      "step 5200: train loss 1.4286, val loss 1.7496, train perplexity 4.1729, val perplexity 5.7526\n",
      "step 5300: train loss 1.4100, val loss 1.7284, train perplexity 4.0958, val perplexity 5.6318\n",
      "step 5400: train loss 1.4339, val loss 1.7368, train perplexity 4.1951, val perplexity 5.6789\n",
      "step 5500: train loss 1.4280, val loss 1.7588, train perplexity 4.1703, val perplexity 5.8052\n",
      "step 5600: train loss 1.4442, val loss 1.7952, train perplexity 4.2387, val perplexity 6.0204\n",
      "step 5700: train loss 1.3711, val loss 1.7843, train perplexity 3.9397, val perplexity 5.9555\n",
      "step 5800: train loss 1.4049, val loss 1.8233, train perplexity 4.0751, val perplexity 6.1920\n",
      "step 5900: train loss 1.3748, val loss 1.7542, train perplexity 3.9545, val perplexity 5.7791\n",
      "step 5999: train loss 1.3774, val loss 1.7835, train perplexity 3.9646, val perplexity 5.9504\n",
      "Training with config: n_embd=256, n_head=8, n_layer=8, dropout=0.3, learning rate = 0.0003\n",
      "step 0: train loss 4.5261, val loss 3.9201, train perplexity 92.3935, val perplexity 50.4051\n",
      "step 100: train loss 2.6002, val loss 2.6022, train perplexity 13.4662, val perplexity 13.4935\n",
      "step 200: train loss 2.5486, val loss 2.5656, train perplexity 12.7889, val perplexity 13.0084\n",
      "step 300: train loss 2.5197, val loss 2.5447, train perplexity 12.4254, val perplexity 12.7392\n",
      "step 400: train loss 2.4882, val loss 2.4757, train perplexity 12.0395, val perplexity 11.8901\n",
      "step 500: train loss 2.4200, val loss 2.4300, train perplexity 11.2462, val perplexity 11.3588\n",
      "step 600: train loss 2.3777, val loss 2.3758, train perplexity 10.7797, val perplexity 10.7592\n",
      "step 700: train loss 2.3046, val loss 2.3420, train perplexity 10.0204, val perplexity 10.4024\n",
      "step 800: train loss 2.2872, val loss 2.2621, train perplexity 9.8476, val perplexity 9.6035\n",
      "step 900: train loss 2.2294, val loss 2.2685, train perplexity 9.2946, val perplexity 9.6648\n",
      "step 1000: train loss 2.1774, val loss 2.2148, train perplexity 8.8237, val perplexity 9.1593\n",
      "step 1100: train loss 2.1467, val loss 2.1814, train perplexity 8.5565, val perplexity 8.8589\n",
      "step 1200: train loss 2.1192, val loss 2.1658, train perplexity 8.3246, val perplexity 8.7213\n",
      "step 1300: train loss 2.1061, val loss 2.1042, train perplexity 8.2158, val perplexity 8.2009\n",
      "step 1400: train loss 2.0183, val loss 2.0935, train perplexity 7.5255, val perplexity 8.1133\n",
      "step 1500: train loss 2.0078, val loss 2.0219, train perplexity 7.4471, val perplexity 7.5525\n",
      "step 1600: train loss 1.9713, val loss 2.0227, train perplexity 7.1798, val perplexity 7.5584\n",
      "step 1700: train loss 1.9741, val loss 1.9977, train perplexity 7.2000, val perplexity 7.3720\n",
      "step 1800: train loss 1.9445, val loss 2.0199, train perplexity 6.9900, val perplexity 7.5374\n",
      "step 1900: train loss 1.9171, val loss 1.9726, train perplexity 6.8012, val perplexity 7.1892\n",
      "step 2000: train loss 1.9049, val loss 1.9714, train perplexity 6.7188, val perplexity 7.1806\n",
      "step 2100: train loss 1.8742, val loss 1.9530, train perplexity 6.5155, val perplexity 7.0498\n",
      "step 2200: train loss 1.8435, val loss 1.9473, train perplexity 6.3183, val perplexity 7.0097\n",
      "step 2300: train loss 1.7997, val loss 1.9596, train perplexity 6.0480, val perplexity 7.0962\n",
      "step 2400: train loss 1.8051, val loss 1.9414, train perplexity 6.0808, val perplexity 6.9688\n",
      "step 2500: train loss 1.7804, val loss 1.9012, train perplexity 5.9321, val perplexity 6.6941\n",
      "step 2600: train loss 1.7849, val loss 1.8667, train perplexity 5.9591, val perplexity 6.4670\n",
      "step 2700: train loss 1.7424, val loss 1.8917, train perplexity 5.7108, val perplexity 6.6307\n",
      "step 2800: train loss 1.7431, val loss 1.8859, train perplexity 5.7148, val perplexity 6.5924\n",
      "step 2900: train loss 1.7350, val loss 1.8961, train perplexity 5.6687, val perplexity 6.6597\n",
      "step 3000: train loss 1.7493, val loss 1.8646, train perplexity 5.7505, val perplexity 6.4535\n",
      "step 3100: train loss 1.7531, val loss 1.8675, train perplexity 5.7723, val perplexity 6.4724\n",
      "step 3200: train loss 1.7028, val loss 1.8420, train perplexity 5.4895, val perplexity 6.3089\n",
      "step 3300: train loss 1.6914, val loss 1.8293, train perplexity 5.4273, val perplexity 6.2292\n",
      "step 3400: train loss 1.6510, val loss 1.8114, train perplexity 5.2122, val perplexity 6.1188\n",
      "step 3500: train loss 1.6695, val loss 1.8251, train perplexity 5.3096, val perplexity 6.2033\n",
      "step 3600: train loss 1.6659, val loss 1.7966, train perplexity 5.2904, val perplexity 6.0290\n",
      "step 3700: train loss 1.6740, val loss 1.8025, train perplexity 5.3333, val perplexity 6.0645\n",
      "step 3800: train loss 1.6742, val loss 1.8120, train perplexity 5.3347, val perplexity 6.1228\n",
      "step 3900: train loss 1.6551, val loss 1.8092, train perplexity 5.2336, val perplexity 6.1056\n",
      "step 4000: train loss 1.6449, val loss 1.7734, train perplexity 5.1807, val perplexity 5.8907\n",
      "step 4100: train loss 1.6479, val loss 1.7843, train perplexity 5.1959, val perplexity 5.9556\n",
      "step 4200: train loss 1.6202, val loss 1.7790, train perplexity 5.0542, val perplexity 5.9239\n",
      "step 4300: train loss 1.6002, val loss 1.8112, train perplexity 4.9540, val perplexity 6.1178\n",
      "step 4400: train loss 1.5964, val loss 1.7967, train perplexity 4.9351, val perplexity 6.0297\n",
      "step 4500: train loss 1.5494, val loss 1.7820, train perplexity 4.7085, val perplexity 5.9417\n",
      "step 4600: train loss 1.5797, val loss 1.7702, train perplexity 4.8534, val perplexity 5.8721\n",
      "step 4700: train loss 1.5856, val loss 1.7595, train perplexity 4.8822, val perplexity 5.8093\n",
      "step 4800: train loss 1.5823, val loss 1.7486, train perplexity 4.8660, val perplexity 5.7465\n",
      "step 4900: train loss 1.5521, val loss 1.8075, train perplexity 4.7212, val perplexity 6.0949\n",
      "step 5000: train loss 1.5794, val loss 1.7158, train perplexity 4.8522, val perplexity 5.5613\n",
      "step 5100: train loss 1.5965, val loss 1.7736, train perplexity 4.9359, val perplexity 5.8919\n",
      "step 5200: train loss 1.5386, val loss 1.7234, train perplexity 4.6579, val perplexity 5.6037\n",
      "step 5300: train loss 1.5433, val loss 1.7146, train perplexity 4.6799, val perplexity 5.5547\n",
      "step 5400: train loss 1.5320, val loss 1.7556, train perplexity 4.6276, val perplexity 5.7869\n",
      "step 5500: train loss 1.5005, val loss 1.7421, train perplexity 4.4840, val perplexity 5.7093\n",
      "step 5600: train loss 1.5312, val loss 1.7175, train perplexity 4.6239, val perplexity 5.5707\n",
      "step 5700: train loss 1.4879, val loss 1.7609, train perplexity 4.4279, val perplexity 5.8179\n",
      "step 5800: train loss 1.5157, val loss 1.7445, train perplexity 4.5528, val perplexity 5.7233\n",
      "step 5900: train loss 1.4757, val loss 1.7087, train perplexity 4.3743, val perplexity 5.5218\n",
      "step 5999: train loss 1.4650, val loss 1.7567, train perplexity 4.3277, val perplexity 5.7930\n",
      "Training with config: n_embd=256, n_head=8, n_layer=8, dropout=0.4, learning rate = 0.0003\n",
      "step 0: train loss 4.6015, val loss 3.9966, train perplexity 99.6378, val perplexity 54.4152\n",
      "step 100: train loss 2.6139, val loss 2.6151, train perplexity 13.6517, val perplexity 13.6685\n",
      "step 200: train loss 2.5600, val loss 2.5674, train perplexity 12.9362, val perplexity 13.0314\n",
      "step 300: train loss 2.5369, val loss 2.5517, train perplexity 12.6406, val perplexity 12.8293\n",
      "step 400: train loss 2.5239, val loss 2.5214, train perplexity 12.4767, val perplexity 12.4461\n",
      "step 500: train loss 2.4637, val loss 2.4826, train perplexity 11.7486, val perplexity 11.9724\n",
      "step 600: train loss 2.4460, val loss 2.4120, train perplexity 11.5420, val perplexity 11.1567\n",
      "step 700: train loss 2.3446, val loss 2.3603, train perplexity 10.4290, val perplexity 10.5944\n",
      "step 800: train loss 2.2911, val loss 2.3487, train perplexity 9.8853, val perplexity 10.4723\n",
      "step 900: train loss 2.2884, val loss 2.2882, train perplexity 9.8590, val perplexity 9.8573\n",
      "step 1000: train loss 2.2103, val loss 2.2702, train perplexity 9.1183, val perplexity 9.6810\n",
      "step 1100: train loss 2.2110, val loss 2.2445, train perplexity 9.1253, val perplexity 9.4354\n",
      "step 1200: train loss 2.1818, val loss 2.1909, train perplexity 8.8627, val perplexity 8.9436\n",
      "step 1300: train loss 2.1690, val loss 2.1608, train perplexity 8.7492, val perplexity 8.6784\n",
      "step 1400: train loss 2.1197, val loss 2.1409, train perplexity 8.3284, val perplexity 8.5070\n",
      "step 1500: train loss 2.0924, val loss 2.0925, train perplexity 8.1042, val perplexity 8.1055\n",
      "step 1600: train loss 2.0808, val loss 2.1073, train perplexity 8.0107, val perplexity 8.2260\n",
      "step 1700: train loss 2.0532, val loss 2.0737, train perplexity 7.7931, val perplexity 7.9543\n",
      "step 1800: train loss 2.0130, val loss 2.0654, train perplexity 7.4858, val perplexity 7.8882\n",
      "step 1900: train loss 1.9974, val loss 2.0374, train perplexity 7.3696, val perplexity 7.6710\n",
      "step 2000: train loss 1.9474, val loss 2.0629, train perplexity 7.0102, val perplexity 7.8690\n",
      "step 2100: train loss 1.9376, val loss 2.0317, train perplexity 6.9421, val perplexity 7.6272\n",
      "step 2200: train loss 1.9343, val loss 1.9762, train perplexity 6.9190, val perplexity 7.2151\n",
      "step 2300: train loss 1.9084, val loss 1.9731, train perplexity 6.7426, val perplexity 7.1928\n",
      "step 2400: train loss 1.8635, val loss 1.9632, train perplexity 6.4463, val perplexity 7.1218\n",
      "step 2500: train loss 1.8724, val loss 1.9508, train perplexity 6.5040, val perplexity 7.0342\n",
      "step 2600: train loss 1.8713, val loss 1.9715, train perplexity 6.4965, val perplexity 7.1817\n",
      "step 2700: train loss 1.8284, val loss 1.9384, train perplexity 6.2237, val perplexity 6.9475\n",
      "step 2800: train loss 1.8471, val loss 1.9212, train perplexity 6.3412, val perplexity 6.8294\n",
      "step 2900: train loss 1.8331, val loss 1.8712, train perplexity 6.2534, val perplexity 6.4959\n",
      "step 3000: train loss 1.7892, val loss 1.9032, train perplexity 5.9849, val perplexity 6.7076\n",
      "step 3100: train loss 1.8408, val loss 1.9004, train perplexity 6.3015, val perplexity 6.6888\n",
      "step 3200: train loss 1.7853, val loss 1.8844, train perplexity 5.9615, val perplexity 6.5826\n",
      "step 3300: train loss 1.7559, val loss 1.8543, train perplexity 5.7889, val perplexity 6.3871\n",
      "step 3400: train loss 1.7556, val loss 1.8753, train perplexity 5.7871, val perplexity 6.5231\n",
      "step 3500: train loss 1.7666, val loss 1.8438, train perplexity 5.8508, val perplexity 6.3204\n",
      "step 3600: train loss 1.7151, val loss 1.8328, train perplexity 5.5570, val perplexity 6.2516\n",
      "step 3700: train loss 1.7721, val loss 1.8356, train perplexity 5.8830, val perplexity 6.2689\n",
      "step 3800: train loss 1.7160, val loss 1.8622, train perplexity 5.5620, val perplexity 6.4382\n",
      "step 3900: train loss 1.7246, val loss 1.8063, train perplexity 5.6101, val perplexity 6.0879\n",
      "step 4000: train loss 1.6859, val loss 1.8113, train perplexity 5.3972, val perplexity 6.1186\n",
      "step 4100: train loss 1.6612, val loss 1.8041, train perplexity 5.2657, val perplexity 6.0747\n",
      "step 4200: train loss 1.6814, val loss 1.8313, train perplexity 5.3730, val perplexity 6.2422\n",
      "step 4300: train loss 1.6525, val loss 1.7954, train perplexity 5.2200, val perplexity 6.0217\n",
      "step 4400: train loss 1.6772, val loss 1.7727, train perplexity 5.3505, val perplexity 5.8865\n",
      "step 4500: train loss 1.6560, val loss 1.8043, train perplexity 5.2383, val perplexity 6.0757\n",
      "step 4600: train loss 1.6448, val loss 1.8402, train perplexity 5.1800, val perplexity 6.2981\n",
      "step 4700: train loss 1.6266, val loss 1.7494, train perplexity 5.0867, val perplexity 5.7514\n",
      "step 4800: train loss 1.6237, val loss 1.7958, train perplexity 5.0719, val perplexity 6.0240\n",
      "step 4900: train loss 1.6110, val loss 1.8206, train perplexity 5.0077, val perplexity 6.1753\n",
      "step 5000: train loss 1.5777, val loss 1.7849, train perplexity 4.8439, val perplexity 5.9588\n",
      "step 5100: train loss 1.6170, val loss 1.7665, train perplexity 5.0377, val perplexity 5.8503\n",
      "step 5200: train loss 1.6121, val loss 1.7785, train perplexity 5.0134, val perplexity 5.9212\n",
      "step 5300: train loss 1.5830, val loss 1.7913, train perplexity 4.8698, val perplexity 5.9973\n",
      "step 5400: train loss 1.6101, val loss 1.7682, train perplexity 5.0036, val perplexity 5.8600\n",
      "step 5500: train loss 1.5905, val loss 1.8010, train perplexity 4.9064, val perplexity 6.0560\n",
      "step 5600: train loss 1.5878, val loss 1.7181, train perplexity 4.8929, val perplexity 5.5738\n",
      "step 5700: train loss 1.6195, val loss 1.7732, train perplexity 5.0505, val perplexity 5.8895\n",
      "step 5800: train loss 1.5495, val loss 1.7320, train perplexity 4.7093, val perplexity 5.6518\n",
      "step 5900: train loss 1.5660, val loss 1.7707, train perplexity 4.7875, val perplexity 5.8748\n",
      "step 5999: train loss 1.5817, val loss 1.7836, train perplexity 4.8634, val perplexity 5.9512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Hiperparametry do przetestowania\n",
    "grid_params = {\n",
    "    \"n_embd\": [256],\n",
    "    \"n_head\": [8],\n",
    "    \"n_layer\": [8],\n",
    "    \"dropout\": [0.2, 0.3,0.4],\n",
    "}\n",
    "\n",
    "# Utworzenie przestrzeni hiperparametrów\n",
    "param_combinations = list(itertools.product(\n",
    "    grid_params[\"n_embd\"],\n",
    "    grid_params[\"n_head\"],\n",
    "    grid_params[\"n_layer\"],\n",
    "    grid_params[\"dropout\"]\n",
    "))\n",
    "\n",
    "# Główna pętla Grid Search\n",
    "# Lista do przechowywania wyników\n",
    "output_dir = \"wyniki_grid_search\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for params in param_combinations:\n",
    "    results = []\n",
    "    n_embd, n_head, n_layer, dropout = params\n",
    "    print(f\"Training with config: n_embd={n_embd}, n_head={n_head}, n_layer={n_layer}, dropout={dropout}, learning rate = {learning_rate}\")\n",
    "\n",
    "    # Tworzenie katalogu na wyniki tej konfiguracji\n",
    "    config_dir = f\"{output_dir}/n_embd_{n_embd}_n_head_{n_head}_n_layer_{n_layer}_dropout_{dropout}_lr{learning_rate}\"\n",
    "    os.makedirs(config_dir, exist_ok=True)\n",
    "    \n",
    "    # Inicjalizacja modelu z bieżącą konfiguracją\n",
    "    model = GPTModel().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Listy na wyniki\n",
    "    train_losses, val_losses, perplexities_train, perplexities_val = [], [], [], []\n",
    "\n",
    "    # Pętla treningowa\n",
    "    for iter in range(max_iters):\n",
    "        # Dane treningowe\n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zapis strat i perplexity\n",
    "        train_losses.append(loss.item())\n",
    "        perplexities_train.append(calculate_perplexity(logits, yb))\n",
    "\n",
    "        # Dane walidacyjne\n",
    "        X_val, Y_val = get_batch('val')\n",
    "        logits_val, _ = model(X_val, Y_val)\n",
    "        val_loss = F.cross_entropy(logits_val.view(-1, logits_val.size(-1)), Y_val.view(-1)).item()\n",
    "        val_losses.append(val_loss)\n",
    "        perplexity_val = calculate_perplexity(logits_val, Y_val)\n",
    "        perplexities_val.append(perplexity_val)\n",
    "\n",
    "        # Drukowanie metryk\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            print(f\"step {iter}: train loss {train_losses[-1]:.4f}, val loss {val_losses[-1]:.4f}, \"\n",
    "                  f\"train perplexity {perplexities_train[-1]:.4f}, val perplexity {perplexity_val:.4f}\")\n",
    "        # Zapis wyników\n",
    "        result = {\n",
    "            \"n_embd\": n_embd,\n",
    "            \"n_head\": n_head,\n",
    "            \"n_layer\": n_layer,\n",
    "            \"dropout\": dropout,\n",
    "            \"final_train_loss\": train_losses[-1],\n",
    "            \"final_val_loss\": val_losses[-1],\n",
    "            \"final_train_perplexity\": perplexities_train[-1],\n",
    "            \"final_val_perplexity\": perplexities_val[-1],\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Zapis wykresów\n",
    "    plt.figure()\n",
    "    plt.plot(perplexities_train, label='Nieokreśloność zbiór treningowy', color='blue')\n",
    "    plt.plot(perplexities_val, label='Nieokreśloność zbiór walidacyjny', color='orange')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteracja')\n",
    "    plt.ylabel('Nieokreśloność')\n",
    "    plt.title('Nieokreśloność zbiór treningowy vs walidacyjny')\n",
    "    plt.savefig(os.path.join(config_dir, 'perplexity.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Strata zbiór treningowy', color='blue')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteracja')\n",
    "    plt.ylabel('Strata')\n",
    "    plt.title('Strata treningowa w czasie')\n",
    "    plt.savefig(os.path.join(config_dir, 'train_loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(val_losses, label='Strata zbiór walidacyjny', color='orange')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteracja')\n",
    "    plt.ylabel('Strata')\n",
    "    plt.title('Strata walidacyjna w czasie')\n",
    "    plt.savefig(os.path.join(config_dir, 'val_loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Zapis wygenerowanego tekstu\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    generated_text = decode(model.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "    with open(os.path.join(config_dir, 'generated_text.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(generated_text)\n",
    "\n",
    "    # Zapis wyników do pliku CSV\n",
    "    csv_file = os.path.join(config_dir, \"grid_search_results.csv\")\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        \n",
    "        writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
